look you are profisional in code editing and AI model formation , at the same time your ability in correction the irregularity in the file you see the general schematic diagram or tree of my project help to finish it  and regarding the above project i have these file and code 
But your replaying me in Arabic to understand well and be ware its a huge project so dont herry and the quality of project is the priority , and step by step , after each step is finish tested then we shift to new step 
the general tree of project was 

problem_ai_advisor/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ raw/                    # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â”œâ”€â”€ processed/              # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
â”‚   â””â”€â”€ models/                 # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ data_processing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database_connector.py    # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â”‚   â”œâ”€â”€ data_extractor.py        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â”‚   â””â”€â”€ data_preprocessor.py     # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ clustering_model.py      # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
â”‚   â”‚   â”œâ”€â”€ topic_modeling.py        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
â”‚   â”‚   â””â”€â”€ pattern_mining.py        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
â”‚   â”œâ”€â”€ ğŸ“ analysis/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ problem_analyzer.py      # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
â”‚   â”‚   â””â”€â”€ recommendation_engine.py # Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª
â”‚   â””â”€â”€ ğŸ“ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ text_processing.py       # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
â”‚       â””â”€â”€ visualization.py         # Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb    # Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â”œâ”€â”€ 02_model_training.ipynb      # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
â”‚   â””â”€â”€ 03_results_analysis.ipynb    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
â”œâ”€â”€ ğŸ“ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI application
â”‚   â””â”€â”€ endpoints.py                 # API endpoints
â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”œâ”€â”€ dashboard.py                 # Streamlit dashboard
â”‚   â””â”€â”€ components/
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_data_processing.py
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database_config.py
â”‚   â””â”€â”€ model_config.py
â”œâ”€â”€ requirements.txt                 # Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
â”œâ”€â”€ setup.py                        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
â”œâ”€â”€ README.md                       # ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
â””â”€â”€ main.py                         # Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

the database that i used are 

C:\Users\pc\PycharmProjects\pythonProject\problem_management_ststem\instance>dir
 Volume in drive C is OS
 Volume Serial Number is BE2A-ADAB

 Directory of C:\Users\pc\PycharmProjects\pythonProject\problem_management_ststem\instance

06/04/2025  08:56 AM    <DIR>          .
06/04/2025  10:42 AM    <DIR>          ..
06/04/2025  08:56 AM            81,920 problem_management.db
               1 File(s)         81,920 bytes
               2 Dir(s)  221,178,884,096 bytes free

C:\Users\pc\PycharmProjects\pythonProject\problem_management_ststem\instance>

# Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©:
- K-Means Clustering
- DBSCAN (Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØºÙŠØ± Ù…Ù†ØªØ¸Ù…Ø©)
- Hierarchical Clustering
- Gaussian Mixture Models
# Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª:
- Latent Dirichlet Allocation (LDA)
- Non-Negative Matrix Factorization (NMF)
- BERTopic (Ø§Ù„Ø£Ø­Ø¯Ø« ÙˆØ§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©)
# Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª:
- Association Rules (Apriori Algorithm)
- Sequential Pattern Mining
- Markov Chains
# 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
- ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ©
- ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ vectors Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ø£Ùˆ BERT
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©

# 2. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª
- Ø­Ø³Ø§Ø¨ Ù…Ø¯Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
- Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„Ø­Ù„ÙˆÙ„ Ù„ÙƒÙ„ Ù…Ø¬Ø§Ù„
- ØªÙƒÙ„ÙØ© Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø©
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
# 1. Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

# 2. Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª
from sklearn.decomposition import LatentDirichletAllocation

# 3. Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
from mlxtend.frequent_patterns import apriori

Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„ØªÙŠ Ø³ÙŠÙ‚Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©:
1. ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©

"Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ØªØ´Ø¨Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø±Ù‚Ù… X Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ø¹Ø§Ø¯Ø© Ø¥Ù„Ù‰ Y Ø£ÙŠØ§Ù… Ù„Ù„Ø­Ù„"
"Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© ØªÙƒÙ„Ù ÙÙŠ Ø§Ù„Ù…ØªÙˆØ³Ø· Z Ø¯ÙˆÙ„Ø§Ø±"

2. ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø­Ù„ÙˆÙ„

"Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ø­ÙÙ„Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±Ù‚..."
"Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„Ø­Ù„ÙˆÙ„ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù‡Ùˆ X%"

3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±

"Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„Ù‡Ø§ Ù…Ø®Ø§Ø·Ø± Ù…Ø´ØªØ±ÙƒØ© Ù…Ø«Ù„..."
"Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù†Ø§Ø¬Ø­Ø© Ø¹Ø§Ø¯Ø© ØªØªØ¬Ù†Ø¨..."

4. ØªØ®ØµÙŠØµ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯

"Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙŠØ­ØªØ§Ø¬ ÙØ±ÙŠÙ‚ Ù…Ù† X Ø£Ø´Ø®Ø§Øµ"
"Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©"

Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠ:
Ù‡Ù„ ØªØ±ÙŠØ¯ Ù…Ù†ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„ÙŠ ÙŠÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ©:

ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ÙÙŠØ©
Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©

# Data Processing
pandas>=1.5.0
numpy>=1.21.0
sqlalchemy>=1.4.0
psycopg2-binary>=2.9.0  # for PostgreSQL
pymysql>=1.0.0          # for MySQL

# Machine Learning
scikit-learn>=1.1.0
xgboost>=1.6.0
lightgbm>=3.3.0

# Text Processing
nltk>=3.7
spacy>=3.4.0
transformers>=4.20.0
sentence-transformers>=2.2.0

# Topic Modeling
gensim>=4.2.0
bertopic>=0.11.0

# Clustering & Pattern Mining
hdbscan>=0.8.0
mlxtend>=0.20.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.9.0
wordcloud>=1.8.0

# API & Dashboard
fastapi>=0.80.0
streamlit>=1.12.0
uvicorn>=0.18.0

# Utilities
python-dotenv>=0.20.0
pydantic>=1.9.0
tqdm>=4.64.0
joblib>=1.1.0

# config/database_config.py
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'problem_management',
    'username': 'your_username',
    'password': 'your_password'
}

# Tables mapping
TABLES = {
    'problems': 'problem',
    'solutions': 'proposed_solution',
    'implementations': 'implementation_plan',
    'kpis': 'kpi_measurement',
    'lessons': 'lesson_learned'
}

# config/model_config.py
MODEL_CONFIG = {
    'clustering': {
        'n_clusters': 8,
        'algorithm': 'kmeans',
        'features': ['title', 'description_initial', 'domain', 'complexity_level']
    },
    'topic_modeling': {
        'n_topics': 10,
        'method': 'bertopic',
        'language': 'multilingual'  # supports Arabic
    },
    'text_processing': {
        'max_features': 1000,
        'min_df': 2,
        'max_df': 0.95,
        'ngram_range': (1, 2)
    }
}

# src/data_processing/database_connector.py
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine, text
from typing import Dict, List, Optional
import logging
from config.database_config import DATABASE_CONFIG, TABLES

class DatabaseConnector:
    """
    ÙƒÙ„Ø§Ø³ Ù„Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    
    def __init__(self, config: Dict = None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        self.config = config or DATABASE_CONFIG
        self.engine = None
        self.connect()
        
    def connect(self):
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ connection string
            conn_string = f"postgresql://{self.config['username']}:{self.config['password']}@{self.config['host']}:{self.config['port']}/{self.config['database']}"
            
            self.engine = create_engine(conn_string)
            logging.info("ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise
    
    def extract_problems_data(self, limit: int = None) -> pd.DataFrame:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
        """
        query = """
        SELECT 
            p.id as problem_id,
            p.title,
            p.description_initial,
            p.domain,
            p.complexity_level,
            p.date_identified,
            p.date_closed,
            p.status,
            p.stakeholders_involved,
            p.initial_impact_assessment,
            p.problem_source,
            p.refined_problem_statement_final,
            p.sentiment_score,
            p.sentiment_label,
            p.problem_tags,
            p.ai_generated_summary,
            
            -- Ù…Ù† Ø¬Ø¯ÙˆÙ„ ÙÙ‡Ù… Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
            pu.active_listening_notes,
            pu.key_questions_asked,
            pu.initial_data_sources,
            pu.initial_hypotheses,
            pu.stakeholder_feedback_initial,
            
            -- Ù…Ù† Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨
            ca.data_collection_methods_deep,
            ca.data_analysis_techniques_used,
            ca.key_findings_from_analysis,
            
            -- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ø®ØªØ§Ø±
            cs.justification_for_choice,
            cs.approval_status,
            cs.date_chosen,
            
            -- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­
            ps.solution_description,
            ps.generation_method,
            ps.estimated_cost,
            ps.estimated_time_to_implement,
            ps.potential_benefits,
            ps.potential_risks,
            
            -- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ†ÙÙŠØ°
            ip.plan_description,
            ip.overall_status as implementation_status,
            ip.start_date_planned,
            ip.end_date_planned,
            ip.start_date_actual,
            ip.end_date_actual,
            ip.overall_budget,
            ip.key_personnel,
            
            -- Ø§Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ù…Ø³ØªÙØ§Ø¯Ø©
            ll.what_went_well,
            ll.what_could_be_improved,
            ll.recommendations_for_future,
            ll.key_takeaways
            
        FROM problem p
        LEFT JOIN problem_understanding pu ON p.id = pu.problem_id
        LEFT JOIN cause_analysis ca ON p.id = ca.problem_id
        LEFT JOIN chosen_solution cs ON p.id = cs.problem_id
        LEFT JOIN proposed_solution ps ON cs.proposed_solution_id = ps.id
        LEFT JOIN implementation_plan ip ON cs.id = ip.chosen_solution_id
        LEFT JOIN lesson_learned ll ON p.id = ll.problem_id
        """
        
        if limit:
            query += f" LIMIT {limit}"
            
        try:
            df = pd.read_sql(query, self.engine)
            logging.info(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(df)} Ù…Ø´ÙƒÙ„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            return df
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise
    
    def extract_kpi_data(self) -> pd.DataFrame:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        """
        query = """
        SELECT 
            sk.chosen_solution_id,
            sk.kpi_name,
            sk.kpi_description,
            sk.target_value,
            sk.current_value_baseline,
            sk.measurement_unit,
            sk.measurement_frequency,
            km.measurement_date,
            km.actual_value,
            km.notes
        FROM solution_kpi sk
        LEFT JOIN kpi_measurement km ON sk.id = km.kpi_id
        ORDER BY sk.chosen_solution_id, km.measurement_date
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            logging.info(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(df)} Ù‚ÙŠØ§Ø³ Ù…Ø¤Ø´Ø± Ø£Ø¯Ø§Ø¡")
            return df
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª: {e}")
            raise
    
    def extract_root_causes(self) -> pd.DataFrame:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠØ©
        """
        query = """
        SELECT 
            prc.analysis_id,
            ca.problem_id,
            prc.cause_description,
            prc.evidence_supporting_cause,
            prc.validation_status,
            prc.impact_of_cause
        FROM potential_root_cause prc
        JOIN cause_analysis ca ON prc.analysis_id = ca.id
        """
        
        try:
            df = pd.read_sql(query, self.engine)
            logging.info(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(df)} Ø³Ø¨Ø¨ Ø¬Ø°Ø±ÙŠ")
            return df
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠØ©: {e}")
            raise
    
    def get_database_stats(self) -> Dict:
        """
        Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        stats = {}
        
        try:
            # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            result = pd.read_sql("SELECT COUNT(*) as count FROM problem", self.engine)
            stats['total_problems'] = result['count'].iloc[0]
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„ÙˆÙ„
            result = pd.read_sql("SELECT COUNT(*) as count FROM proposed_solution", self.engine)
            stats['total_solutions'] = result['count'].iloc[0]
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­Ù„ÙˆÙ„Ø©
            result = pd.read_sql("SELECT COUNT(*) as count FROM problem WHERE status = 'closed'", self.engine)
            stats['solved_problems'] = result['count'].iloc[0]
            
            # Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            result = pd.read_sql("SELECT COUNT(DISTINCT domain) as count FROM problem", self.engine)
            stats['unique_domains'] = result['count'].iloc[0]
            
            logging.info(f"Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {stats}")
            return stats
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")
            raise
    
    def close_connection(self):
        """
        Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        """
        if self.engine:
            self.engine.dispose()
            logging.info("ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(level=logging.INFO)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    db = DatabaseConnector()
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = db.get_database_stats()
    print("Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:", stats)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    problems_df = db.extract_problems_data(limit=100)  # Ø£ÙˆÙ„ 100 Ù…Ø´ÙƒÙ„Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    print(f"ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(problems_df)} Ù…Ø´ÙƒÙ„Ø©")
    print("Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:", problems_df.columns.tolist())
    
    # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„
    db.close_connection()


# src/models/clustering_model.py
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import joblib
import logging
from datetime import datetime

class ProblemClusteringModel:
    """
    Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ù…ÙØ´Ø±Ù Ø¹Ù„ÙŠÙ‡
    """
    
    def __init__(self, n_clusters: int = 8, clustering_method: str = 'kmeans'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            n_clusters: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            clustering_method: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ('kmeans', 'dbscan')
        """
        self.n_clusters = n_clusters
        self.clustering_method = clustering_method
        self.vectorizer = None
        self.scaler = None
        self.clustering_model = None
        self.label_encoders = {}
        self.feature_names = []
        self.cluster_info = {}
        
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        
        Args:
            df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            
        Returns:
            np.ndarray: Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª...")
        
        features_list = []
        
        # 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
        text_columns = ['title', 'description_initial', 'problem_tags', 
                       'stakeholders_involved', 'refined_problem_statement_final']
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ
        combined_text = df[text_columns].fillna('').agg(' '.join, axis=1)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ vectors
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                max_features=500,
                min_df=2,
                max_df=0.95,
                ngram_range=(1, 2),
                stop_words='english'  # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            )
            text_features = self.vectorizer.fit_transform(combined_text).toarray()
        else:
            text_features = self.vectorizer.transform(combined_text).toarray()
            
        features_list.append(text_features)
        
        # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        categorical_columns = ['domain', 'complexity_level', 'status', 'problem_source', 'sentiment_label']
        
        for col in categorical_columns:
            if col in df.columns:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    encoded = self.label_encoders[col].fit_transform(df[col].fillna('unknown'))
                else:
                    encoded = self.label_encoders[col].transform(df[col].fillna('unknown'))
                    
                features_list.append(encoded.reshape(-1, 1))
        
        # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        numeric_columns = ['sentiment_score']
        
        for col in numeric_columns:
            if col in df.columns:
                values = df[col].fillna(df[col].median()).values.reshape(-1, 1)
                features_list.append(values)
        
        # 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_columns = ['date_identified', 'date_closed']
        
        for col in date_columns:
            if col in df.columns:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù… Ù…Ù† ØªØ§Ø±ÙŠØ® Ù…Ø¹ÙŠÙ†
                df[col] = pd.to_datetime(df[col], errors='coerce')
                base_date = pd.to_datetime('2020-01-01')
                days_diff = (df[col] - base_date).dt.days.fillna(0).values.reshape(-1, 1)
                features_list.append(days_diff)
        
        # 5. Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
        text_length = combined_text.str.len().values.reshape(-1, 1)
        features_list.append(text_length)
        
        # Ø¹Ø¯Ø¯ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ù…ØµÙ„Ø­Ø© (ØªÙ‚Ø¯ÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ§ØµÙ„)
        stakeholder_count = df['stakeholders_involved'].fillna('').str.count(',').values.reshape(-1, 1) + 1
        features_list.append(stakeholder_count)
        
        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        all_features = np.hstack(features_list)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³
        if self.scaler is None:
            self.scaler = StandardScaler()
            scaled_features = self.scaler.fit_transform(all_features)
        else:
            scaled_features = self.scaler.transform(all_features)
        
        logging.info(f"ØªÙ… ØªØ­Ø¶ÙŠØ± {scaled_features.shape[1]} Ù…ÙŠØ²Ø© Ù„Ù€ {scaled_features.shape[0]} Ù…Ø´ÙƒÙ„Ø©")
        
        return scaled_features
    
    def train(self, df: pd.DataFrame) -> Dict:
        """
        ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
        
        Args:
            df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            
        Returns:
            Dict: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if self.clustering_method == 'kmeans':
            self.clustering_model = KMeans(
                n_clusters=self.n_clusters, 
                random_state=42,
                n_init=10
            )
        elif self.clustering_method == 'dbscan':
            self.clustering_model = DBSCAN(
                eps=0.5,
                min_samples=5
            )
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        cluster_labels = self.clustering_model.fit_predict(features)
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©
        if len(set(cluster_labels)) > 1:
            silhouette_avg = silhouette_score(features, cluster_labels)
        else:
            silhouette_avg = 0
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        df_with_clusters = df.copy()
        df_with_clusters['cluster'] = cluster_labels
        
        self.cluster_info = self._analyze_clusters(df_with_clusters)
        
        training_info = {
            'n_samples': len(df),
            'n_features': features.shape[1],
            'n_clusters_found': len(set(cluster_labels)),
            'silhouette_score': silhouette_avg,
            'cluster_sizes': pd.Series(cluster_labels).value_counts().to_dict(),
            'training_date': datetime.now().isoformat()
        }
        
        logging.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {training_info['n_clusters_found']}")
        logging.info(f"Ù†ØªÙŠØ¬Ø© Silhouette: {silhouette_avg:.3f}")
        
        return training_info
    
    def _analyze_clusters(self, df_with_clusters: pd.DataFrame) -> Dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
        """
        cluster_analysis = {}
        
        for cluster_id in df_with_clusters['cluster'].unique():
            if cluster_id == -1:  # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ DBSCAN
                continue
                
            cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster_id]
            
            analysis = {
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(df_with_clusters) * 100,
                'common_domains': cluster_data['domain'].mode().tolist(),
                'common_complexity': cluster_data['complexity_level'].mode().tolist(),
                'avg_sentiment': cluster_data['sentiment_score'].mean() if 'sentiment_score' in cluster_data else None,
                'common_status': cluster_data['status'].mode().tolist(),
                'sample_titles': cluster_data['title'].head(3).tolist()
            }
            
            cluster_analysis[f'cluster_{cluster_id}'] = analysis
        
        return cluster_analysis
    
    def predict_cluster(self, problem_data: Dict) -> Tuple[int, Dict]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©
        
        Args:
            problem_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            
        Returns:
            Tuple[int, Dict]: Ø±Ù‚Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
        df_new = pd.DataFrame([problem_data])
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df_new)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        cluster_id = self.clustering_model.predict(features)[0]
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
        cluster_info = self.cluster_info.get(f'cluster_{cluster_id}', {})
        
        prediction_info = {
            'cluster_id': int(cluster_id),
            'cluster_info': cluster_info,
            'confidence': self._calculate_confidence(features, cluster_id)
        }
        
        return cluster_id, prediction_info
    
    def _calculate_confidence(self, features: np.ndarray, predicted_cluster: int) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤
        """
        if self.clustering_method == 'kmeans':
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ù…Ø±ÙƒØ² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            cluster_center = self.clustering_model.cluster_centers_[predicted_cluster]
            distance = np.linalg.norm(features[0] - cluster_center)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø«Ù‚Ø© (ÙƒÙ„Ù…Ø§ Ù‚Ù„Øª Ø§Ù„Ù…Ø³Ø§ÙØ© Ø²Ø§Ø¯Øª Ø§Ù„Ø«Ù‚Ø©)
            max_distance = np.max([np.linalg.norm(center - cluster_center) 
                                  for center in self.clustering_model.cluster_centers_])
            confidence = max(0, 1 - (distance / max_distance))
            
            return float(confidence)
        
        return 0.5  # Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„Ø·Ø±Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰
    
    def visualize_clusters(self, df: pd.DataFrame, save_path: str = None):
        """
        Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        """
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df)
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù„Ù„Ø±Ø³Ù…
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(features)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        cluster_labels = self.clustering_model.predict(features)
        
        # Ø§Ù„Ø±Ø³Ù…
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                            c=cluster_labels, cmap='tab10', alpha=0.7)
        plt.colorbar(scatter)
        plt.title('Problem Clusters Visualization')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def save_model(self, file_path: str):
        """
        Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        model_data = {
            'clustering_model': self.clustering_model,
            'vectorizer': self.vectorizer,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'cluster_info': self.cluster_info,
            'n_clusters': self.n_clusters,
            'clustering_method': self.clustering_method
        }
        
        joblib.dump(model_data, file_path)
        logging.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {file_path}")
    
    def load_model(self, file_path: str):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        model_data = joblib.load(file_path)
        
        self.clustering_model = model_data['clustering_model']
        self.vectorizer = model_data['vectorizer']
        self.scaler = model_data['scaler']
        self.label_encoders = model_data['label_encoders']
        self.cluster_info = model_data['cluster_info']
        self.n_clusters = model_data['n_clusters']
        self.clustering_method = model_data['clustering_method']
        
        logging.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {file_path}")

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(level=logging.INFO)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
    clustering_

# src/models/clustering_model.py
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import joblib
import logging
from datetime import datetime

class ProblemClusteringModel:
    """
    Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ù…ÙØ´Ø±Ù Ø¹Ù„ÙŠÙ‡
    """
    
    def __init__(self, n_clusters: int = 8, clustering_method: str = 'kmeans'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            n_clusters: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            clustering_method: Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ('kmeans', 'dbscan')
        """
        self.n_clusters = n_clusters
        self.clustering_method = clustering_method
        self.vectorizer = None
        self.scaler = None
        self.clustering_model = None
        self.label_encoders = {}
        self.feature_names = []
        self.cluster_info = {}
        
    def prepare_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
        
        Args:
            df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            
        Returns:
            np.ndarray: Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª...")
        
        features_list = []
        
        # 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
        text_columns = ['title', 'description_initial', 'problem_tags', 
                       'stakeholders_involved', 'refined_problem_statement_final']
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ
        combined_text = df[text_columns].fillna('').agg(' '.join, axis=1)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ vectors
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                max_features=500,
                min_df=2,
                max_df=0.95,
                ngram_range=(1, 2),
                stop_words='english'  # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            )
            text_features = self.vectorizer.fit_transform(combined_text).toarray()
        else:
            text_features = self.vectorizer.transform(combined_text).toarray()
            
        features_list.append(text_features)
        
        # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        categorical_columns = ['domain', 'complexity_level', 'status', 'problem_source', 'sentiment_label']
        
        for col in categorical_columns:
            if col in df.columns:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    encoded = self.label_encoders[col].fit_transform(df[col].fillna('unknown'))
                else:
                    encoded = self.label_encoders[col].transform(df[col].fillna('unknown'))
                    
                features_list.append(encoded.reshape(-1, 1))
        
        # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        numeric_columns = ['sentiment_score']
        
        for col in numeric_columns:
            if col in df.columns:
                values = df[col].fillna(df[col].median()).values.reshape(-1, 1)
                features_list.append(values)
        
        # 4. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_columns = ['date_identified', 'date_closed']
        
        for col in date_columns:
            if col in df.columns:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù… Ù…Ù† ØªØ§Ø±ÙŠØ® Ù…Ø¹ÙŠÙ†
                df[col] = pd.to_datetime(df[col], errors='coerce')
                base_date = pd.to_datetime('2020-01-01')
                days_diff = (df[col] - base_date).dt.days.fillna(0).values.reshape(-1, 1)
                features_list.append(days_diff)
        
        # 5. Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
        text_length = combined_text.str.len().values.reshape(-1, 1)
        features_list.append(text_length)
        
        # Ø¹Ø¯Ø¯ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ù…ØµÙ„Ø­Ø© (ØªÙ‚Ø¯ÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙˆØ§ØµÙ„)
        stakeholder_count = df['stakeholders_involved'].fillna('').str.count(',').values.reshape(-1, 1) + 1
        features_list.append(stakeholder_count)
        
        # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        all_features = np.hstack(features_list)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³
        if self.scaler is None:
            self.scaler = StandardScaler()
            scaled_features = self.scaler.fit_transform(all_features)
        else:
            scaled_features = self.scaler.transform(all_features)
        
        logging.info(f"ØªÙ… ØªØ­Ø¶ÙŠØ± {scaled_features.shape[1]} Ù…ÙŠØ²Ø© Ù„Ù€ {scaled_features.shape[0]} Ù…Ø´ÙƒÙ„Ø©")
        
        return scaled_features
    
    def train(self, df: pd.DataFrame) -> Dict:
        """
        ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
        
        Args:
            df: DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            
        Returns:
            Dict: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if self.clustering_method == 'kmeans':
            self.clustering_model = KMeans(
                n_clusters=self.n_clusters, 
                random_state=42,
                n_init=10
            )
        elif self.clustering_method == 'dbscan':
            self.clustering_model = DBSCAN(
                eps=0.5,
                min_samples=5
            )
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        cluster_labels = self.clustering_model.fit_predict(features)
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©
        if len(set(cluster_labels)) > 1:
            silhouette_avg = silhouette_score(features, cluster_labels)
        else:
            silhouette_avg = 0
        
        # Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        df_with_clusters = df.copy()
        df_with_clusters['cluster'] = cluster_labels
        
        self.cluster_info = self._analyze_clusters(df_with_clusters)
        
        training_info = {
            'n_samples': len(df),
            'n_features': features.shape[1],
            'n_clusters_found': len(set(cluster_labels)),
            'silhouette_score': silhouette_avg,
            'cluster_sizes': pd.Series(cluster_labels).value_counts().to_dict(),
            'training_date': datetime.now().isoformat()
        }
        
        logging.info(f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {training_info['n_clusters_found']}")
        logging.info(f"Ù†ØªÙŠØ¬Ø© Silhouette: {silhouette_avg:.3f}")
        
        return training_info
    
    def _analyze_clusters(self, df_with_clusters: pd.DataFrame) -> Dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
        """
        cluster_analysis = {}
        
        for cluster_id in df_with_clusters['cluster'].unique():
            if cluster_id == -1:  # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ ÙÙŠ DBSCAN
                continue
                
            cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster_id]
            
            analysis = {
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(df_with_clusters) * 100,
                'common_domains': cluster_data['domain'].mode().tolist(),
                'common_complexity': cluster_data['complexity_level'].mode().tolist(),
                'avg_sentiment': cluster_data['sentiment_score'].mean() if 'sentiment_score' in cluster_data else None,
                'common_status': cluster_data['status'].mode().tolist(),
                'sample_titles': cluster_data['title'].head(3).tolist()
            }
            
            cluster_analysis[f'cluster_{cluster_id}'] = analysis
        
        return cluster_analysis
    
    def predict_cluster(self, problem_data: Dict) -> Tuple[int, Dict]:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©
        
        Args:
            problem_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            
        Returns:
            Tuple[int, Dict]: Ø±Ù‚Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ DataFrame
        df_new = pd.DataFrame([problem_data])
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df_new)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        cluster_id = self.clustering_model.predict(features)[0]
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
        cluster_info = self.cluster_info.get(f'cluster_{cluster_id}', {})
        
        prediction_info = {
            'cluster_id': int(cluster_id),
            'cluster_info': cluster_info,
            'confidence': self._calculate_confidence(features, cluster_id)
        }
        
        return cluster_id, prediction_info
    
    def _calculate_confidence(self, features: np.ndarray, predicted_cluster: int) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤
        """
        if self.clustering_method == 'kmeans':
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ù…Ø±ÙƒØ² Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            cluster_center = self.clustering_model.cluster_centers_[predicted_cluster]
            distance = np.linalg.norm(features[0] - cluster_center)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø«Ù‚Ø© (ÙƒÙ„Ù…Ø§ Ù‚Ù„Øª Ø§Ù„Ù…Ø³Ø§ÙØ© Ø²Ø§Ø¯Øª Ø§Ù„Ø«Ù‚Ø©)
            max_distance = np.max([np.linalg.norm(center - cluster_center) 
                                  for center in self.clustering_model.cluster_centers_])
            confidence = max(0, 1 - (distance / max_distance))
            
            return float(confidence)
        
        return 0.5  # Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„Ø·Ø±Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰
    
    def visualize_clusters(self, df: pd.DataFrame, save_path: str = None):
        """
        Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        """
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features = self.prepare_features(df)
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù„Ù„Ø±Ø³Ù…
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(features)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        cluster_labels = self.clustering_model.predict(features)
        
        # Ø§Ù„Ø±Ø³Ù…
        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                            c=cluster_labels, cmap='tab10', alpha=0.7)
        plt.colorbar(scatter)
        plt.title('Problem Clusters Visualization')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        plt.show()
    
    def save_model(self, file_path: str):
        """
        Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        model_data = {
            'clustering_model': self.clustering_model,
            'vectorizer': self.vectorizer,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'cluster_info': self.cluster_info,
            'n_clusters': self.n_clusters,
            'clustering_method': self.clustering_method
        }
        
        joblib.dump(model_data, file_path)
        logging.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {file_path}")
    
    def load_model(self, file_path: str):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        model_data = joblib.load(file_path)
        
        self.clustering_model = model_data['clustering_model']
        self.vectorizer = model_data['vectorizer']
        self.scaler = model_data['scaler']
        self.label_encoders = model_data['label_encoders']
        self.cluster_info = model_data['cluster_info']
        self.n_clusters = model_data['n_clusters']
        self.clustering_method = model_data['clustering_method']
        
        logging.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {file_path}")

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(level=logging.INFO)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹
    clustering_model = ProblemClusteringModel(n_clusters=8, clustering_method='kmeans')
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø¨Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„ÙØ¹Ù„ÙŠØ©)
    # Ù…Ù† database_connector
    from src.data_processing.database_connector import DatabaseConnector
    
    db = DatabaseConnector()
    problems_df = db.extract_problems_data()
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    training_results = clustering_model.train(problems_df)
    print("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:", training_results)
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    clustering_model.save_model('data/models/clustering_model.pkl')
    
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©
    new_problem = {
        'title': 'Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©',
        'description_initial': 'Ø§Ù†Ù‚Ø·Ø§Ø¹ Ù…ØªÙƒØ±Ø± ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª',
        'domain': 'IT',
        'complexity_level': 'Medium',
        'sentiment_score': -0.5,
        'sentiment_label': 'Negative',
        'stakeholders_involved': 'IT Team, Management',
        'problem_tags': 'network, connectivity, infrastructure'
    }
    
    cluster_id, prediction_info = clustering_model.predict_cluster(new_problem)
    print(f"Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ØªÙ†ØªÙ…ÙŠ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {cluster_id}")
    print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤: {prediction_info}")
    
    # Ø±Ø³Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
    clustering_model.visualize_clusters(problems_df, save_path='data/cluster_visualization.png')
    
    db.close_connection()

# src/analysis/recommendation_engine.py
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import logging
from datetime import datetime, timedelta
import statistics

class ManagementRecommendationEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø¥Ø¯Ø§Ø±Ø© - ÙŠÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    
    def __init__(self, clustering_model=None):
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        
        Args:
            clustering_model: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø±Ø¨
        """
        self.clustering_model = clustering_model
        self.problems_data = None
        self.solutions_data = None
        self.success_patterns = {}
        
    def load_historical_data(self, problems_df: pd.DataFrame, kpi_df: pd.DataFrame = None):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„
        
        Args:
            problems_df: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            kpi_df: Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        """
        self.problems_data = problems_df.copy()
        self.kpi_data = kpi_df
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø¬Ø§Ø­
        self._analyze_success_patterns()
        
        logging.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(problems_df)} Ù…Ø´ÙƒÙ„Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„")
    
    def _analyze_success_patterns(self):
        """
        ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠ Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        """
        if self.problems_data is None:
            return
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø¬Ø§Ø­ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ø§Ù„
        domain_success = {}
        for domain in self.problems_data['domain'].unique():
            if pd.isna(domain):
                continue
                
            domain_problems = self.problems_data[self.problems_data['domain'] == domain]
            solved_problems = domain_problems[domain_problems['status'] == 'closed']
            
            if len(domain_problems) > 0:
                success_rate = len(solved_problems) / len(domain_problems)
                avg_time_to_solve = self._calculate_avg_resolution_time(solved_problems)
                avg_cost = self._estimate_avg_cost(solved_problems)
                
                domain_success[domain] = {
                    'success_rate': success_rate,
                    'avg_resolution_days': avg_time_to_solve,
                    'avg_cost_estimate': avg_cost,
                    'total_problems': len(domain_problems),
                    'solved_problems': len(solved_problems)
                }
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù†Ø¬Ø§Ø­ Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_success = {}
        for complexity in self.problems_data['complexity_level'].unique():
            if pd.isna(complexity):
                continue
                
            complexity_problems = self.problems_data[self.problems_data['complexity_level'] == complexity]
            solved_problems = complexity_problems[complexity_problems['status'] == 'closed']
            
            if len(complexity_problems) > 0:
                success_rate = len(solved_problems) / len(complexity_problems)
                avg_time_to_solve = self._calculate_avg_resolution_time(solved_problems)
                
                complexity_success[complexity] = {
                    'success_rate': success_rate,
                    'avg_resolution_days': avg_time_to_solve,
                    'total_problems': len(complexity_problems),
                    'solved_problems': len(solved_problems)
                }
        
        self.success_patterns = {
            'by_domain': domain_success,
            'by_complexity': complexity_success,
            'overall_stats': self._calculate_overall_stats()
        }
    
    def _calculate_avg_resolution_time(self, solved_problems: pd.DataFrame) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø­Ù„
        """
        if len(solved_problems) == 0:
            return 0
        
        resolution_times = []
        for _, problem in solved_problems.iterrows():
            if pd.notna(problem['date_identified']) and pd.notna(problem['date_closed']):
                start_date = pd.to_datetime(problem['date_identified'])
                end_date = pd.to_datetime(problem['date_closed'])
                days_diff = (end_date - start_date).days
                if days_diff > 0:
                    resolution_times.append(days_diff)
        
        return statistics.mean(resolution_times) if resolution_times else 0
    
    def _estimate_avg_cost(self, solved_problems: pd.DataFrame) -> str:
        """
        ØªÙ‚Ø¯ÙŠØ± Ù…ØªÙˆØ³Ø· Ø§Ù„ØªÙƒÙ„ÙØ©
        """
        # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø© - ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„ÙØ¹Ù„ÙŠØ©
        costs = []
        for _, problem in solved_problems.iterrows():
            if pd.notna(problem['estimated_cost']):
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù… Ù…Ù† Ø§Ù„Ù†Øµ
                cost_str = str(problem['estimated_cost']).lower()
                if 'low' in cost_str:
                    costs.append(1000)
                elif 'medium' in cost_str:
                    costs.append(5000)
                elif 'high' in cost_str:
                    costs.append(15000)
        
        if costs:
            avg_cost = statistics.mean(costs)
            if avg_cost < 2000:
                return "Low ($1K-$2K)"
            elif avg_cost < 8000:
                return "Medium ($2K-$8K)"
            else:
                return "High ($8K+)"
        
        return "Unknown"
    
    def _calculate_overall_stats(self) -> Dict:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
        """
        if self.problems_data is None:
            return {}
        
        total_problems = len(self.problems_data)
        solved_problems = len(self.problems_data[self.problems_data['status'] == 'closed'])
        
        return {
            'total_problems': total_problems,
            'solved_problems': solved_problems,
            'overall_success_rate': solved_problems / total_problems if total_problems > 0 else 0,
            'avg_sentiment': self.problems_data['sentiment_score'].mean(),
            'most_common_domain': self.problems_data['domain'].mode().iloc[0] if len(self.problems_data['domain'].mode()) > 0 else 'Unknown'
        }
    
    def analyze_new_problem(self, problem_data: Dict) -> Dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© ÙˆØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªÙˆØµÙŠØ§Øª
        
        Args:
            problem_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            
        Returns:
            Dict: Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
        """
        recommendations = {
            'problem_analysis': {},
            'similar_problems': [],
            'recommended_solutions': [],
            'resource_estimation': {},
            'risk_assessment': {},
            'success_probability': 0.0,
            'management_recommendations': []
        }
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        recommendations['problem_analysis'] = self._analyze_problem_characteristics(problem_data)
        
        # 2. Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø´Ø§ÙƒÙ„ Ù…Ø´Ø§Ø¨Ù‡Ø©
        recommendations['similar_problems'] = self._find_similar_problems(problem_data)
        
        # 3. ØªÙˆØµÙŠØ© Ø§Ù„Ø­Ù„ÙˆÙ„
        recommendations['recommended_solutions'] = self._recommend_solutions(problem_data)
        
        # 4. ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
        recommendations['resource_estimation'] = self._estimate_resources(problem_data)
        
        # 5. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        recommendations['risk_assessment'] = self._assess_risks(problem_data)
        
        # 6. Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ù†Ø¬Ø§Ø­
        recommendations['success_probability'] = self._calculate_success_probability(problem_data)
        
        # 7. ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©
        recommendations['management_recommendations'] = self._generate_management_recommendations(
            problem_data, recommendations
        )
        
        return recommendations
    
    def _analyze_problem_characteristics(self, problem_data: Dict) -> Dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
        """
        analysis = {
            'complexity_analysis': 'Unknown',
            'domain_statistics': {},
            'sentiment_analysis': 'Neutral',
            'urgency_level': 'Medium'
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ø§Ù„
        domain = problem_data.get('domain', 'Unknown')
        if domain in self.success_patterns['by_domain']:
            analysis['domain_statistics'] = self.success_patterns['by_domain'][domain]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment_score = problem_data.get('sentiment_score', 0)
        if sentiment_score < -0.3:
            analysis['sentiment_analysis'] = 'Negative - ÙŠØªØ·Ù„Ø¨ Ø§Ù‡ØªÙ…Ø§Ù… Ø¹Ø§Ø¬Ù„'
        elif sentiment_score > 0.3:
            analysis['sentiment_analysis'] = 'Positive - Ù…Ø´ÙƒÙ„Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø­Ù„'
        else:
            analysis['sentiment_analysis'] = 'Neutral - ØªØ­ØªØ§Ø¬ ØªÙ‚ÙŠÙŠÙ… Ø£Ø¹Ù…Ù‚'
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„
        if 'urgent' in str(problem_data.get('title', '')).lower() or \
           'critical' in str(problem_data.get('description_initial', '')).lower():
            analysis['urgency_level'] = 'High'
        elif sentiment_score < -0.5:
            analysis['urgency_level'] = 'High'
        
        return analysis
    
    def _find_similar_problems(self, problem_data: Dict, top_k: int = 5) -> List[Dict]:
        """
        Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø´Ø§ÙƒÙ„ Ù…Ø´Ø§Ø¨Ù‡Ø©
        """
        if self.problems_data is None or len(self.problems_data) == 0:
            return []
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ù…Ø±ÙƒØ¨ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        new_problem_text = f"{problem_data.get('title', '')} {problem_data.get('description_initial', '')} {problem_data.get('problem_tags', '')}"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØµÙˆØµ Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
        existing_problems_text = []
        for _, row in self.problems_data.iterrows():
            text = f"{row.get('title', '')} {row.get('description_initial', '')} {row.get('problem_tags', '')}"
            existing_problems_text.append(text)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        all_texts = [new_problem_text] + existing_problems_text
        vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
        
        try:
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù…Ø´Ø§ÙƒÙ„ Ù…ØªØ´Ø§Ø¨Ù‡Ø©
            top_indices = similarities.argsort()[-top_k:][::-1]
            
            similar_problems = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ´Ø§Ø¨Ù‡
                    problem = self.problems_data.iloc[idx]
                    similar_problems.append({
                        'problem_id': problem.get('problem_id', idx),
                        'title': problem.get('title', 'Unknown'),
                        'domain': problem.get('domain', 'Unknown'),
                        'status': problem.get('status', 'Unknown'),
                        'similarity_score': float(similarities[idx]),
                        'solution_description': problem.get('solution_description', 'N/A'),
                        'resolution_time': self._calculate_resolution_time(problem)
                    })
            
            return similar_problems
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø´Ø§ÙƒÙ„ Ù…Ø´Ø§Ø¨Ù‡Ø©: {e}")
            return []
    
    def _calculate_resolution_time(self, problem: pd.Series) -> int:
        """
        Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„Ø­Ù„ Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ÙŠÙ†Ø©
        """
        try:
            if pd.notna(problem.get('date_identified')) and pd.notna(problem.get('date_closed')):
                start_date = pd.to_datetime(problem['date_identified'])
                end_date = pd.to_datetime(problem['date_closed'])
                return (end_date - start_date).days
        except:
            pass
        return 0
    
    def _recommend_solutions(self, problem_data: Dict) -> List[Dict]:
        """
        ØªÙˆØµÙŠØ© Ø§Ù„Ø­Ù„ÙˆÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
        """
        similar_problems = self._find_similar_problems(problem_data)
        
        solution_recommendations = []
        solution_counts = {}
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
        for similar_problem in similar_problems:
            solution = similar_problem.get('solution_description', '')
            if solution and solution != 'N/A':
                if solution in solution_counts:
                    solution_counts[solution] += 1
                else:
                    solution_counts[solution] = 1
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø´ÙŠÙˆØ¹
        sorted_solutions = sorted(solution_counts.items(), key=lambda x: x[1], reverse=True)
        
        for solution, count in sorted_solutions[:3]:  # Ø£ÙØ¶Ù„ 3 Ø­Ù„ÙˆÙ„
            solution_recommendations.append({
                'solution_description': solution,
                'frequency': count,
                'confidence': count / len(similar_problems) if similar_problems else 0,
                'based_on_problems': len(similar_problems)
            })
        
        return solution_recommendations
    
    def _estimate_resources(self, problem_data: Dict) -> Dict:
        """
        ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        """
        domain = problem_data.get('domain', 'Unknown')
        complexity = problem_data.get('complexity_level', 'Medium')
        
        estimation = {
            'estimated_time_days': 30,  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
            'estimated_cost': 'Medium',
            'required_team_size': 3,
            'estimated_confidence': 0.5
        }
        
        # ØªÙ‚Ø¯ÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        if domain in self.success_patterns['by_domain']:
            domain_stats = self.success_patterns['by_domain'][domain]
            estimation['estimated_time_days'] = int(domain_stats['avg_resolution_days'])
            estimation['estimated_cost'] = domain_stats['avg_cost_estimate']
            estimation['estimated_confidence'] = 0.8
        
        if complexity in self.success_patterns['by_complexity']:
            complexity_stats = self.success_patterns['by_complexity'][complexity]
            if estimation['estimated_time_days'] == 30:  # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù…Ù† Ø§Ù„Ù…Ø¬Ø§Ù„
                estimation['estimated_time_days'] = int(complexity_stats['avg_resolution_days'])
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ÙØ±ÙŠÙ‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity == 'High':
            estimation['required_team_size'] = 5
        elif complexity == 'Low':
            estimation['required_team_size'] = 2
        
        return estimation
    
    def _assess_risks(self, problem_data: Dict) -> Dict:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        """
        risks = {
            'overall_risk_level': 'Medium',
            'identified_risks': [],
            'mitigation_suggestions': []
        }
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        domain = problem_data.get('domain', 'Unknown')
        complexity = problem_data.get('complexity_level', 'Medium')
        sentiment = problem_data.get('sentiment_score', 0)
        
        # Ù…Ø®Ø§Ø·Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ø§Ù„
        if domain in self.success_patterns['by_domain']:
            success_rate = self.success_patterns['by_domain'][domain]['success_rate']
            if success_rate < 0.6:
                risks['identified_risks'].append(f"Ø§Ù„Ù…Ø¬Ø§Ù„ {domain} Ù„Ù‡ Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ù…Ù†Ø®ÙØ¶ ({success_rate:.1%})")
                risks['overall_risk_level'] = 'High'
        
        # Ù…Ø®Ø§Ø·Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity == 'High':
            risks['identified_risks'].append("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ø§Ù„ÙŠ ÙŠØ²ÙŠØ¯ Ù…Ù† Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªØ£Ø®ÙŠØ±")
            risks['overall_risk_level'] = 'High'
        
        # Ù…Ø®Ø§Ø·Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        if sentiment < -0.5:
            risks['identified_risks'].append("Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø§Ù„Ù‚ÙˆÙŠØ© Ù‚Ø¯ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø§ÙˆÙ†")
            risks['mitigation_suggestions'].append("ÙŠÙ†ØµØ­ Ø¨ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ù…ØµÙ„Ø­Ø©")
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¹Ø§Ù…Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        risks['mitigation_suggestions'].extend([
            "ÙˆØ¶Ø¹ Ø®Ø·Ø© Ø·ÙˆØ§Ø±Ø¦ ÙˆØ§Ø¶Ø­Ø©",
            "Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù… Ø¨Ø§Ù†ØªØ¸Ø§Ù…",
            "Ø¶Ù…Ø§Ù† ØªÙˆÙØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù„Ø§Ø²Ù…Ø©"
        ])
        
        return risks
    
    def _calculate_success_probability(self, problem_data: Dict) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ù†Ø¬Ø§Ø­
        """
        probability = 0.7  # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        
        domain = problem_data.get('domain', 'Unknown')
        complexity = problem_data.get('complexity_level', 'Medium')
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ø§Ù„
        if domain in self.success_patterns['by_domain']:
            domain_success_rate = self.success_patterns['by_domain'][domain]['success_rate']
            probability = (probability + domain_success_rate) / 2
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity == 'Low':
            probability += 0.1
        elif complexity == 'High':
            probability -= 0.2
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment = problem_data.get('sentiment_score', 0)
        if sentiment < -0.5:
            probability -= 0.1
        elif sentiment > 0.3:
            probability += 0.1
        
        return max(0.1, min(0.95, probability))
    
    def _generate_management_recommendations(self, problem_data: Dict, analysis: Dict) -> List[str]:
        """
        Ø¥Ù†ØªØ§Ø¬ ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©
        """
        recommendations = []
        
        success_prob = analysis['success_probability']
        risk_level = analysis['risk_assessment']['overall_risk_level']
        estimated_time = analysis['resource_estimation']['estimated_time_days']
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ù†Ø¬Ø§Ø­
        if success_prob > 0.8:
            recommendations.append("âœ… Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù†Ø¬Ø§Ø­ Ø¹Ø§Ù„ÙŠØ© - ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©")
        elif success_prob < 0.5:
            recommendations.append("âš ï¸ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù†Ø¬Ø§Ø­ Ù…Ù†Ø®ÙØ¶Ø© - ÙŠØªØ·Ù„Ø¨ Ø¥Ø´Ø±Ø§Ù Ø¥Ø¯Ø§Ø±ÙŠ Ù…ÙƒØ«Ù")
            recommendations.append("ğŸ’¡ Ø§Ù„Ù†Ø¸Ø± ÙÙŠ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ±")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        if risk_level == 'High':
            recommendations.append("ğŸš¨ Ù…Ø³ØªÙˆÙ‰ Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠ - ØªØ®ØµÙŠØµ Ù…ÙˆØ§Ø±Ø¯ Ø¥Ø¶Ø§ÙÙŠØ©")
            recommendations.append("ğŸ“Š Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© Ù„Ù„ØªÙ‚Ø¯Ù…")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
        if estimated_time > 60:
            recommendations.append("â° Ù…Ø´Ø±ÙˆØ¹ Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰ - ÙˆØ¶Ø¹ Ù…Ø¹Ø§Ù„Ù… Ø²Ù…Ù†ÙŠØ© ÙˆØ§Ø¶Ø­Ø©")
            recommendations.append("ğŸ‘¥ Ø§Ù„Ù†Ø¸Ø± ÙÙŠ ØªØ´ÙƒÙŠÙ„ ÙØ±ÙŠÙ‚ Ù…ØªØ®ØµØµ")
        elif estimated_time < 7:
            recommendations.append("âš¡ Ø­Ù„ Ø³Ø±ÙŠØ¹ Ù…ØªÙˆÙ‚Ø¹ - Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ù„Ù„ØªÙ†ÙÙŠØ°")
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
        similar_problems = analysis['similar_problems']
        if len(similar_problems) > 0:
            avg_similarity = sum(p['similarity_score'] for p in similar_problems) / len(similar_problems)
            if avg_similarity > 0.7:
                recommendations.append("ğŸ”„ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø¬Ø±Ø¨Ø© Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©")
        else:
            recommendations.append("ğŸ†• Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø© - Ø§Ù„Ù†Ø¸Ø± ÙÙŠ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø®Ø¨Ø±Ø§Ø¡ Ø®Ø§Ø±Ø¬ÙŠÙŠÙ†")
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
        recommendations.append("ğŸ“ ØªÙˆØ«ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹")
        recommendations.append("ğŸ¯ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ ÙˆØ§Ø¶Ø­Ø© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†Ø¬Ø§Ø­")
        
        return recommendations

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
    logging.basicConfig(level=logging.INFO)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙˆØµÙŠØ§Øª
    recommendation_engine = ManagementRecommendationEngine()
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© (Ù…Ø«Ø§Ù„)
    # ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø¨Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„ÙØ¹Ù„ÙŠØ©
    sample_data = pd.DataFrame({
        'problem_id': range(1, 101),
        'title': ['Problem ' + str(i) for i in range(1, 101)],
        'domain': ['IT', 'HR', 'Finance'] * 34,
        'status': ['closed', 'open'] * 50,
        'complexity_level': ['Low', 'Medium', 'High'] * 34,
        'sentiment_score': np.random.uniform(-1, 1, 100)
    })
    
    recommendation_engine.load_historical_data(sample_data)
    
    # ØªØ­Ù„ÙŠÙ„ Ù…Ø´ÙƒÙ„Ø© Ø¬Ø¯ÙŠØ¯Ø©
    new_problem = {
        'title': 'Server Performance Issue',
        'description_initial': 'Database server experiencing slow response times',
        'domain': 'IT',
        'complexity_level': 'High',
        'sentiment_score': -0.6,
        'stakeholders_involved': 'IT Team, Management, End Users'
    }
    
    analysis_results = recommendation_engine.analyze_new_problem(new_problem)
    
    print("=== ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ===")
    print(f"Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ù†Ø¬Ø§Ø­: {analysis_results['success_probability']:.1%}")
    print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±: {analysis_results['risk_assessment']['overall_risk_level']}")
    print(f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {analysis_results['resource_estimation']['estimated_time_days']} ÙŠÙˆÙ…")
    
    print("\n=== ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© ===")
    for i, recommendation in enumerate(analysis_results['management_recommendations'], 1):
        print(f"{i}. {recommendation}")



# frontend/dashboard.py
import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import sys
import os

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data_processing.database_connector import DatabaseConnector
from src.models.clustering_model import ProblemClusteringModel
from src.analysis.recommendation_engine import ManagementRecommendationEngine
import logging

# ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="AI Problem Management Dashboard",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ø¥Ø®ÙØ§Ø¡ ØªØ­Ø°ÙŠØ±Ø§Øª streamlit
st.set_option('deprecation.showPyplotGlobalUse', False)

# CSS Ù…Ø®ØµØµ
st.markdown("""
<style>
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 1rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin-bottom: 2rem;
}

.metric-card {
    background: white;
    padding: 1rem;
    border-radius: 10px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    border-left: 4px solid #667eea;
}

.recommendation-card {
    background: #f8f9fa;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #28a745;
    margin: 0.5rem 0;
}

.risk-high { border-left-color: #dc3545 !important; }
.risk-medium { border-left-color: #ffc107 !important; }
.risk-low { border-left-color: #28a745 !important; }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
<div class="main-header">
    <h1>ğŸ¤– Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø´Ø§ÙƒÙ„</h1>
    <p>ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ ÙˆØªÙˆØµÙŠØ§Øª Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¯Ø§Ø±Ø©</p>
</div>
""", unsafe_allow_html=True)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø©
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
    st.session_state.clustering_model = None
    st.session_state.recommendation_engine = None
    st.session_state.problems_data = None

# Sidebar
st.sidebar.title("âš™ï¸


""""""""""""""""""""""""""""
after you see the tree and principal help in complitiona any file defect as a single file or as un generated file


# src/utils/text_processing.py
import re
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø¬Ø°Ø± Ø¹Ø±Ø¨ÙŠ
# Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ nltk Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
try:
    stopwords.words('arabic')
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


# Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
ARABIC_STOPWORDS = set(stopwords.words('arabic'))
ENGLISH_STOPWORDS = set(stopwords.words('english'))
ALL_STOPWORDS = ARABIC_STOPWORDS.union(ENGLISH_STOPWORDS)

# ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù…Ø®ØµØµØ© Ø¥Ø°Ø§ Ù„Ø§Ø­Ø¸Øª ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ÙŠÙ†Ø© ØªØªÙƒØ±Ø± Ø¨ÙƒØ«Ø±Ø© ÙˆÙ„ÙŠØ³Øª Ù…ÙÙŠØ¯Ø©
CUSTOM_STOPWORDS = {"Ù…Ø«Ù„", "Ø§ÙŠØ¶Ø§", "ÙƒØ§Ù†", "ÙŠÙƒÙˆÙ†", "Ø£Ùˆ", "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø§Ù„Ù‰", "Ø¹Ù„ÙŠ", "Ø­ØªÙŠ", "Ø§Ù„Ø®", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠ"}
ALL_STOPWORDS.update(CUSTOM_STOPWORDS)


def normalize_arabic_text(text: str) -> str:
    """
    ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ØŒ ØªÙˆØ­ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ).
    """
    if not isinstance(text, str):
        return ""
    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ùˆ", text)
    text = re.sub("Ø¦", "ÙŠ", "Ø¡" if text.endswith("Ø¡") else "ÙŠ", text) # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ù‡Ù…Ø²Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ§Ø¡
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("Ú¯", "Ùƒ", text)
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[\u064B-\u0652]', '', text)
    return text

def remove_punctuation_and_digits(text: str) -> str:
    """
    Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù….
    """
    if not isinstance(text, str):
        return ""
    # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù…
    # string.punctuation ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    # Ù†Ø¶ÙŠÙ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    arabic_punctuation = """`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€"""
    all_punctuation = string.punctuation + arabic_punctuation
    translator = str.maketrans('', '', all_punctuation + string.digits)
    return text.translate(translator)

def remove_stopwords(text: str, custom_stopwords_list: set = None) -> str:
    """
    Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù…Ù† Ø§Ù„Ù†Øµ.
    """
    if not isinstance(text, str):
        return ""

    stopwords_to_use = ALL_STOPWORDS
    if custom_stopwords_list:
        stopwords_to_use = stopwords_to_use.union(custom_stopwords_list)

    words = nltk.word_tokenize(text) # Ø§Ø³ØªØ®Ø¯Ø§Ù… nltk.word_tokenize Ù„Ù„ØªÙ‚Ø³ÙŠÙ…
    filtered_words = [word for word in words if word.lower() not in stopwords_to_use and len(word) > 1]
    return " ".join(filtered_words)

def stem_text_arabic(text: str) -> str:
    """
    ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¬Ø°ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ISRIStemmer.
    """
    if not isinstance(text, str):
        return ""
    stemmer = ISRIStemmer()
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return " ".join(stemmed_words)

def preprocess_text_pipeline(text: str, use_stemming: bool = False) -> str:
    """
    Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ ÙƒØ§Ù…Ù„ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: ØªØ·Ø¨ÙŠØ¹ØŒ Ø¥Ø²Ø§Ù„Ø© ØªØ±Ù‚ÙŠÙ… ÙˆØ£Ø±Ù‚Ø§Ù…ØŒ Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø©ØŒ ØªØ¬Ø°ÙŠØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ).
    """
    if not isinstance(text, str) or pd.isna(text): # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† pd.isna Ù…Ù‡Ù…
        return ""

    # 1. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø© (Ù…Ù‡Ù… Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆÙ„ØªÙˆØ­ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ·Ø¨ÙŠØ¹Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„)
    text = text.lower()
    # 2. ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    text = normalize_arabic_text(text)
    # 3. Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù…
    text = remove_punctuation_and_digits(text)
    # 4. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (URLs)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # 5. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª (@username) ÙˆØ§Ù„Ù‡Ø§Ø´ØªØ§Øº (#topic) (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©)
    text = re.sub(r'\@\w+|\#\w+', '', text)
    # 6. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    text = " ".join(text.split())
    # 7. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    text = remove_stopwords(text)
    # 8. Ø§Ù„ØªØ¬Ø°ÙŠØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    if use_stemming:
        text = stem_text_arabic(text) # Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ¬Ø°ÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ

    return text.strip()


# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
if __name__ == '__main__':
    import pandas as pd # Ø¥Ø¶Ø§ÙØ© pandas Ù‡Ù†Ø§ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    sample_arabic_text = "Ø§Ù„Ø³Ù„Ø§Ù…Ù Ø¹Ù„ÙŠÙƒÙ…Ù’ ÙˆØ±Ø­Ù…Ø©Ù Ø§Ù„Ù„Ù‡Ù ÙˆØ¨Ø±ÙƒØ§ØªÙ‡. Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ÙŒ Ø¹Ù„Ù‰ Ù†ØµÙ Ø¹Ø±Ø¨ÙŠÙÙ‘ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§ØªÙ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù…Ø«Ù„ Ùˆ ÙÙŠ Ù…Ù† ÙˆØ¨Ø¹Ø¶ Ø§Ù„ØªØ±Ù‚ÙŠÙ… 123 ØŸ! ÙˆØ§Ù„ØªØ·ÙˆÙŠÙ„Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø§Øª. Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ùˆ http://example.com"
    sample_english_text = "Hello world! This is an example of English text, with stopwords like the and a number 456. @user #topic"

    print("Original Arabic:", sample_arabic_text)
    processed_arabic = preprocess_text_pipeline(sample_arabic_text, use_stemming=True)
    print("Processed Arabic:", processed_arabic)
    print("-" * 30)
    print("Original English:", sample_english_text)
    processed_english = preprocess_text_pipeline(sample_english_text) # Stemming for English would need a different stemmer
    print("Processed English:", processed_english)

    # Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ù‚ÙŠÙ…Ø© None Ø£Ùˆ NaN
    nan_value = None
    processed_nan = preprocess_text_pipeline(nan_value)
    print(f"\nProcessed None: '{processed_nan}' (length: {len(processed_nan)})")

    empty_string = ""
    processed_empty = preprocess_text_pipeline(empty_string)
    print(f"Processed Empty String: '{processed_empty}' (length: {len(processed_empty)})")

    # src/data_processing/data_preprocessor.py
import pandas as pd
import numpy as np
import os
import logging
from datetime import datetime

# ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ØµØ­ÙŠØ­Ø©
try:
    from src.data_processing.database_connector import DatabaseConnector
    from src.utils.text_processing import preprocess_text_pipeline
except ImportError:
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ´ØºÙ„ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ù„Ù… ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† PYTHONPATH
    import sys
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø°Ø± Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ sys.path
    # ÙŠÙØªØ±Ø¶ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ problem_ai_advisor/src/data_processing
    current_dir_preprocessor = os.path.dirname(os.path.abspath(__file__))
    project_root_preprocessor = os.path.abspath(os.path.join(current_dir_preprocessor, '..', '..'))
    if project_root_preprocessor not in sys.path:
        sys.path.insert(0, project_root_preprocessor)
    from src.data_processing.database_connector import DatabaseConnector
    from src.utils.text_processing import preprocess_text_pipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class DataPreprocessor:
    def __init__(self, db_connector: DatabaseConnector):
        self.db_connector = db_connector
        self.raw_data = None
        self.processed_data = None

    def load_data(self, limit: int = None) -> pd.DataFrame:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…...")
        try:
            self.raw_data = self.db_connector.extract_problems_data(limit=limit)
            logging.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.raw_data)} Ø³Ø¬Ù„ Ø®Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­.")
            # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
            logging.info(f"Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…: {self.raw_data.shape}")
            logging.info(f"Ø£ÙˆÙ„ 3 ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…:\n{self.raw_data.head(3)}")
            logging.info(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©:\n{self.raw_data.info()}")
            return self.raw_data
        except Exception as e:
            logging.error(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…: {e}")
            raise

    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©.
        """
        logging.info("Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©...")
        # Ù…Ø«Ø§Ù„: Ù…Ù„Ø¡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù†Øµ ÙØ§Ø±ØºØŒ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨Ø§Ù„ÙˆØ³ÙŠØ· Ø£Ùˆ Ø§Ù„Ù…ØªÙˆØ³Ø·
        for col in df.columns:
            if df[col].dtype == 'object' or pd.api.types.is_string_dtype(df[col]):
                df[col] = df[col].fillna('') # Ù…Ù„Ø¡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù†Øµ ÙØ§Ø±Øº
            elif pd.api.types.is_numeric_dtype(df[col]):
                # ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙˆØ³ÙŠØ· Ø£Ùˆ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø£Ùˆ Ù‚ÙŠÙ…Ø© Ø«Ø§Ø¨ØªØ©
                df[col] = df[col].fillna(df[col].median()) # Ù…Ù„Ø¡ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø§Ù„ÙˆØ³ÙŠØ·

        # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        # Ù…Ø«Ø§Ù„: df['sentiment_score'] = df['sentiment_score'].fillna(0)
        logging.info("Ø§ÙƒØªÙ…Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©.")
        return df

    def _convert_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø«Ù„Ø§Ù‹ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙˆØ§Ø±ÙŠØ® Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù…).
        """
        logging.info("Ø¨Ø¯Ø¡ ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

        # ØªØ­ÙˆÙŠÙ„ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        date_columns = ['date_identified', 'date_closed', 'date_chosen',
                        'start_date_planned', 'end_date_planned',
                        'start_date_actual', 'end_date_actual']
        for col in date_columns:
            if col in df.columns:
                # errors='coerce' Ø³ÙŠØ­ÙˆÙ„ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø© Ø¥Ù„Ù‰ NaT (Not a Time)
                df[col] = pd.to_datetime(df[col], errors='coerce')
                logging.info(f"ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ '{col}' Ø¥Ù„Ù‰ datetime.")

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø±Ù‚Ù…ÙŠØ© (Ù…Ø«Ø§Ù„: sentiment_score)
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª sentiment_score Ù…Ù‚Ø±ÙˆØ¡Ø© ÙƒÙ†ØµØŒ Ø³ØªØ­ØªØ§Ø¬ Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§
        if 'sentiment_score' in df.columns and not pd.api.types.is_numeric_dtype(df['sentiment_score']):
             # errors='coerce' Ø³ÙŠØ­ÙˆÙ„ Ø§Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø¥Ù„Ù‰ NaN
            df['sentiment_score'] = pd.to_numeric(df['sentiment_score'], errors='coerce')
            logging.info("ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ 'sentiment_score' Ø¥Ù„Ù‰ Ø±Ù‚Ù…ÙŠ.")


        # **Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Ø«Ù„ ØªÙƒÙ„ÙØ© Ø£Ùˆ ÙˆÙ‚Øª**
        # Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙŠØ¹ØªÙ…Ø¯ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©.
        # Ø³Ù†Ø­ØªØ§Ø¬ Ù„Ø£Ù…Ø«Ù„Ø© Ù…Ù†Ùƒ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…Ø«Ù„ 'estimated_cost', 'estimated_time_to_implement'
        # Ù„Ù†ÙØªØ±Ø¶ Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø£Ù† 'estimated_cost' Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙƒÙ†ØµÙˆØµ Ø£Ùˆ Ù†ØµÙˆØµ Ù…Ø«Ù„ "Ù…Ù†Ø®ÙØ¶"
        # Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· Ø¬Ø¯Ø§Ù‹ (ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ):
        if 'estimated_cost' in df.columns:
            # ÙŠÙ…ÙƒÙ†Ùƒ Ù‡Ù†Ø§ ÙƒØªØ§Ø¨Ø© Ø¯Ø§Ù„Ø© Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
            # df['estimated_cost_numeric'] = df['estimated_cost'].apply(lambda x: extract_numeric_cost(x))
            logging.warning("Ø§Ù„Ø¹Ù…ÙˆØ¯ 'estimated_cost' Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ù†Ø·Ù‚ ØªØ­ÙˆÙŠÙ„ Ù…Ø®ØµØµ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ø±Ù‚Ù…ÙŠ.")

        if 'overall_budget' in df.columns:
             logging.warning("Ø§Ù„Ø¹Ù…ÙˆØ¯ 'overall_budget' Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ù†Ø·Ù‚ ØªØ­ÙˆÙŠÙ„ Ù…Ø®ØµØµ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ø±Ù‚Ù…ÙŠ.")


        logging.info("Ø§ÙƒØªÙ…Ù„ ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return df

    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Ù‡Ù†Ø¯Ø³Ø© Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.
        """
        logging.info("Ø¨Ø¯Ø¡ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª...")

        # Ø­Ø³Ø§Ø¨ Ù…Ø¯Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© (Ø¨Ø§Ù„Ø£ÙŠØ§Ù…)
        if 'date_identified' in df.columns and 'date_closed' in df.columns:
            # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…Ù† Ù†ÙˆØ¹ datetime ÙˆØ£Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù…Ø«Ù„Ø§Ù‹ NaT)
            df['resolution_time_days'] = (df['date_closed'] - df['date_identified']).dt.days
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙƒÙˆÙ† ÙÙŠÙ‡Ø§ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ù‚Ø¨Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ø¯ÙŠØ¯ Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚ÙŠÙ… Ù…ÙÙ‚ÙˆØ¯Ø©
            df['resolution_time_days'] = df['resolution_time_days'].apply(lambda x: x if pd.notna(x) and x >= 0 else np.nan)
            logging.info("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø© 'resolution_time_days'.")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø© Ù†ØµÙŠØ© Ù…Ø¯Ù…Ø¬Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„
        # Ù‡Ø°Ù‡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‚ØªØ±Ø­Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§
        text_fields_to_combine = [
            'title', 'description_initial', 'refined_problem_statement_final',
            'stakeholders_involved', 'initial_impact_assessment', 'problem_source',
            'active_listening_notes', 'key_questions_asked', 'initial_hypotheses',
            'key_findings_from_analysis', 'potential_root_causes_list', # Ù‡Ø°Ø§ Ù…Ù† Ø§Ù„Ù…ÙØªØ±Ø¶ Ø£Ù† ÙŠÙƒÙˆÙ† Ù…ØªØ§Ø­Ù‹Ø§ Ø§Ù„Ø¢Ù†
            'solution_description', 'justification_for_choice',
            'what_went_well', 'what_could_be_improved', 'recommendations_for_future', 'key_takeaways'
        ]
        # ØªØ£ÙƒØ¯ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ df ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠÙ‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ Ø¨Ù€ '')
        existing_text_fields = [col for col in text_fields_to_combine if col in df.columns]

        logging.info(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø¯Ù…Ø¬Ù‡Ø§: {existing_text_fields}")

        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ù‡Ø§ Ø³Ù„Ø§Ø³Ù„ Ù†ØµÙŠØ©
        df['combined_text_for_nlp'] = df[existing_text_fields].astype(str).agg(' '.join, axis=1)
        logging.info("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø© 'combined_text_for_nlp'.")


        # ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ù…Ø¬
        logging.info("Ø¨Ø¯Ø¡ ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ø¹Ù„Ù‰ 'combined_text_for_nlp'...")
        # Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ù‡Ù†Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… tqdm Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª
        df['processed_text'] = df['combined_text_for_nlp'].apply(lambda x: preprocess_text_pipeline(x, use_stemming=False)) # Ø§Ù„ØªØ¬Ø°ÙŠØ± Ø§Ø®ØªÙŠØ§Ø±ÙŠ
        logging.info("Ø§ÙƒØªÙ…Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù€ 'processed_text'.")


        logging.info("Ø§ÙƒØªÙ…Ù„Øª Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª.")
        return df

    def preprocess(self, limit: int = None, save_processed_data: bool = True,
                   processed_data_path: str = "data/processed/processed_problems_data.csv") -> pd.DataFrame:
        """
        Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„.
        """
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.load_data(limit=limit)
        if self.raw_data is None or self.raw_data.empty:
            logging.error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ù… Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
            return pd.DataFrame()

        df = self.raw_data.copy()

        # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        df = self._handle_missing_values(df)

        # 3. ØªØ­ÙˆÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = self._convert_data_types(df)

        # 4. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ)
        df = self._engineer_features(df)

        self.processed_data = df
        logging.info("Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©.")
        logging.info(f"Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.processed_data.shape}")
        logging.info(f"Ø£ÙˆÙ„ 3 ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©):\n{self.processed_data[['problem_id', 'title', 'processed_text', 'resolution_time_days']].head(3)}")
        logging.info(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:\n{self.processed_data.info()}")


        if save_processed_data:
            try:
                # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ 'data/processed/' Ù…ÙˆØ¬ÙˆØ¯
                os.makedirs(os.path.dirname(processed_data_path), exist_ok=True)
                self.processed_data.to_csv(processed_data_path, index=False, encoding='utf-8-sig') # utf-8-sig Ù„Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ CSV
                logging.info(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ: {processed_data_path}")
            except Exception as e:
                logging.error(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")

        return self.processed_data

# Ù…Ø«Ø§Ù„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
if __name__ == '__main__':
    # Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø³ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ Ø§ØªØµØ§Ù„ ØµØ§Ù„Ø­ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    # ÙˆÙ…Ù„Ù config/database_config.py Ù…Ù‡ÙŠØ£ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
    try:
        db_connector_instance = DatabaseConnector() # ÙŠÙØªØ±Ø¶ Ø£Ù† config Ù…Ù‡ÙŠØ£ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        preprocessor = DataPreprocessor(db_connector=db_connector_instance)

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ù…Ø«Ù„Ø§Ù‹ Ø£ÙˆÙ„ 10 Ù…Ø´Ø§ÙƒÙ„)
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ©ØŒ Ù‡Ø°Ø§ Ù‚Ø¯ Ù„Ø§ ÙŠØ¹ÙŠØ¯ Ø´ÙŠØ¦Ù‹Ø§
        processed_df = preprocessor.preprocess(limit=10)

        if not processed_df.empty:
            print("\n--- Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---")
            print(processed_df[['problem_id', 'title', 'domain', 'resolution_time_days', 'processed_text']].head())
            print("\n--- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---")
            processed_df.info()
        else:
            print("Ù„Ù… ÙŠØªÙ… Ø¥Ù†ØªØ§Ø¬ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø©. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø£Ùˆ Ø­Ø¯ limit.")

    except FileNotFoundError as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª config: {e}")
    except ConnectionError as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    except Exception as e:
        logging.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø®ØªØ¨Ø§Ø± DataPreprocessor: {e}", exc_info=True)
    finally:
        if 'db_connector_instance' in locals() and db_connector_instance.engine:
            db_connector_instance.close_connection()


